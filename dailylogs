 ##########################   docker instalation using conventional script ###############################

link for documentation     ----> https://docs.docker.com/engine/install/ubuntu/#install-using-the-convenience-script 

* To login to docker hub   ----> docker login
* To build a image         ----> docker build . -f <docker-file-name> -t <docker-username>/<app-name>
* To push to docker hub    ----> docker push <docker-username>/<app-name>
* To see runnning containers --> docker ps
* To list all stoped &running -> docker ps -a
* To get container id      ----> docker ps -a -q
* To start container       ----> docker start <cont-id>
* To connect to containe   ----> docker exec -it <cont-name> /bin/sh
* To stop container        ----> docker stop <cont-name>
* To remove container      ----> docker rm   <cont-name>
* To remove image          ----> docker rmi  <image-name> or docker rmi -f <image-name> ## -f for remove forcefully
* To add tag to image      ----> docker tag  <docker-hub-id>/<image-name>:<tag-name>
* To push newly tagged image --> docker push <docker-hub-id>/<image-name>:<tag-name>
* To display livestats     ----> docker stats


* django project 2 docker  ----> https://www.youtube.com/watch?v=W5Ov0H7E_o4
######################### ##  enable ssh-authentcation ###############
* To enable ssh-authentication , first create an ssh keys 
  $ ssh-keygen -t rsa

* After creating successfully, from user directory, open the .ssh folder
  $ cd .ssh
  
* lookout for "id_rsa.pub" file and copy the contents of the file

* Login to github through UI and under settings clickk for SSH AND GPG keys on left hand bar

* Click on New SSH key , provide the title and paste the copied content in the space provided under key and click on Add SSH key


########################### basi-git-commands #######################

* To install git               ---->  sudo apt install git
* To ckeck the version         ---->  git version
* To configure username        ---->  git config --global user.name "git_user_name"
* To configure email           ---->  git config --global user.email "git_user_mail"
* To clone repository          ---->  git clone <git--url>
* To add a remote repo         ---->  git remote add <remote-repo-name> <url-of-source-repo>
* To check status              ---->  git status
* To add to staginng area      ---->  git add <file name>  (or)  git add .      # "." to add all the files in taging area
* To add only modified files 
  leaving untracked files      ---->  git add -u
* To stage all files           ---->  git add -A
* To commit the file/change    ---->  git commit -m "<commit-message>"
* To push to the remote repo   ---->  git push <remote-repo-name> <name-of-branch-to-push>
* To start fresh git repo
  in local/ initialise         ---->  git init 
* To configure username        ---->  git config --global user.name "git_user_name"
* To configure email           ---->  git config --global user.name "git_user_mail"
* To list user                 ---->  git config --global --list    
* To get all tracked files     ---->  git ls-files
* To get logs of commits       ---->  git log (or) git log --abbrev-commit (or) git log --oneline --graph --decorate --all
* To get a logs in between 2 
  commits                      ----> git log <from_commit_id>...<to-commit-id>
* To get commits happend in 
  last n days                  ----> git log --since="<n> days ago"  # n= no of days
* To get logs of a file        ----> git log -- <file_name>
* To get log of renamed        ----> git log -- follow -- <file-path>
* To show about specific commit ---> git show <commit-id>
* To edit the commit message   ----> git commit --amend
* To change url from https to ssh -> git remote set-url origin <--link/url-->
* To rename the repo as in remote 
  repo                          ---> git remote set-url origin <--link/url-->



 
################## git workflow (push,pull) ####################

* To pull from remote repo     ----> git pull <remote-repo-name> <name-of-branch-to-push>
* To push to the remote repo   ----> git push <remote-repo-name> <name-of-branch-to-push>

#################  tracking files    ##################

* To add and commit the 
  tracked file                 ----> git commit -am "<your-message>"   # trackerd file : These are the files previously commited to git

############ backing out changes (undo,rename,move,delete)    ###################

* To backout changes added to 
  staging area                 ----> git reset HEAD <name-of-file>    # To unstage from staging area
* To undo the changes made     ----> git checkout -- <file-name-to-undo-changes>  # this will undo the changes that have been made to a file after the last commit
* To rename file using git     ----> git mv <source_file> <new-file-name>
* To move file using git       ----> git mv <source_file_path> <destination_file_path> 
* To remove tracked file       ----> git rm <file_to_remove>  # the specified file move from committed to staging area and commit to remove 
* To undo delete staged file 
  that is in staging area       ---> git reset HEAD <file_name> # the file will be added to git repo but will not be present in working directory to get it back in wd
* To undo deleted file to 
  working directory             ---> git checkout <filename>
* To add deletetion done  
  directley on working directory --> git add -A # If a staged file is removed/deleted directley from working directory, this will be in staging area, commit to 
                                                  delete permenentley
       
#############  how to set up alias in git ###############

* To create alias              ----> git config --global alias.<alias-name> "<operation-to-perform>" 
                                     ## eg git config --global alias.history "log --all --oneline --graph --decorate"
* To update alias              ----> nano ~/.gitconfig

#############  comparing differences   #############

* To get diff bw staging area 
  and working directory        ----> git diff (or) git diff -- <file-name> # to get the difference of a specific file
* To get diff bw last commit
  and working directory        ----> git diff HEAD
* To get diff bw staged and
  and last commit              ----> git diff --staged HEAD
* To get diff between committs ----> git diff <commit-id1> <comit-id2>
* To compare b/w last 2 commit ----> git diff HEAD HEAD^
* To get diff b/w branches     ----> git diff master origin/master

###########  branching and merging    ########################

* To list local branch         -----> git branch
* To list all branches         -----> git branch -a ## list all local and remote 
* To create a branch           -----> git branch <branch-name>-----------------------|
                                                                                     |-----> to create and switch simontaniously ==> git checkout -b <branch-name>  
* To switch branch             -----> git checkout <branch-name>---------------------|
* To renmane branch            -----> git branch -m <old_name> <new_name>
* To delete branch             -----> git branch -d <old_name> <new_name>
* To merge                     -----> git merge <name-of-the-branch-to-merge> -m "<message>" ## merge with master/main branch and see difference beform merging
* To merge without fast forword ----> git merge <name-of-the-branch-to-merge> --no-ff -m "<message>"
* Merging conflict             -----> 
 
#####################  rebeasing  ############################

* To rebasing to main/master  ------> git rebase <branch-name> # aftre branching out, if u want to get all the updates/modifications from specified branch
* To abort rebase if conflict 
  is raised                   ------> git rebase --abort
* To merge conflicts          ------> git mergetool --tool=<your_merge_tool_name>
* To continue rebase after
  solving conflicts           ------> git rebase --continue

############   stashing   ######################

* Stashing is like pausing, if we want to pause the curent working and work on something important, we can stach the process as it pauses and changes all the files
  to previous commit
* To stash/pause               -----> git stash 
* To undo stash                -----> git stash apply
* To list all stashes          -----> git stash list
* To drop the stash            -----> git stash drop
* To consider even untracked 
  files                        -----> git stash -u
* To combine both apply & drop -----> git stash pop
* To save stash with message   -----> git stash save "<stash_message>"
* To get info of specific stash ----> git stash show stash@{index_num}  # you will get index_num if you do "git stash list"\
* To apply to a specific stash  ----> git stash apply stash@{index_num}
* To drop a specific stash     -----> git stash drop stash@{index_num}
* To delete all stashes        -----> git stash clear
* To switch to new branch     ------> git stash branch <branch_name> # it will create & switch 2 new specified branch and drops the stash 

###################  tagging  ############################

* To create a tag             ------> git tag <tag_name> ##light-weight tag
* To list tags                ------> git tag --list
* To get info of tag          ------> git show <tag_name>
* To delete tag               ------> git tag --delete <tag-name>
* To create annotated tag     ------> git tag -a <tag-name> -m "<tag_message>" ## gives some more info than leight-weight tag
* To diplay diff b/w tags     ------> git diff <first-tag> <second-tag>
* To tagging specific commit  ------> git tag -a <tag-name> -m <tag-msg> <commit-id>
* To update tags              ------> git tag -a <tag-name> -m <message> -f <correct-commit-id>
* To push a specific tag to remote -> git push <remote-repo-name> <tag-name>
* To push all tags to remote  ------> git push <remote_repo_name> <branch_name_2_push> --tags
* To delete a tag in remote repo -->  git push <remote-repo-name> :<tag-name> ## delete tag from repo but will be present in local

############################## reset,reflog and compare branches  ##########################################

* To set head back to previous
  nth commit                   ----> git reset HEAD^(n) git reset HEAD@{n} or   ## n specifies no of commits back u want to go to
* To get history of all the 
  reset commands              -----> git reflog
* To change to a specific 
  commit using commit id      -----> git reset <commit-id> ### u will get id by "git reflog"

####################################### github actions and how to create and clone a repository through cli ###########################################

* Login to github             ------> gh auth login
* Create and clone a repo     ------> gh repo create <git-user-name>/<repo-name> --<public/private> -c  ## "-c" flag to clone the repo
* To run the workflow from cli  ----> gh workflow run <action-filename>.yml
* To list the workflows       ------> gh run list --workflow = <action-file-name>.yml  ## here you will get job id
* To get logs from running job  ----> gh run view --log --job = <job-id> ## get job id from above command


#######################################################################################################################################################################

                                                                       AZURE SQL
#######################################################################################################################################################################

  * How to create a table

CREATE EXTERNAL TABLE <table_name>
(
    <col-name> <datatype> <constraint>,
    <col-name> <datatype> <constraint>,
      :
      :
    <col-name> <datatype> <constraint>
)


  * How to insert value to table

INSERT INTO <TABLE-NAME> ( <col-name>, <col-name>) VALUES ( value1,value2)

  * How to create a table as CTAS
 
CREATE TABLE <destinatio-tablename>
AS
SELECT * FROM <SOURCE-TABLE-NAME> WHERE <condition>



#######################################################################################################################################################################

                                                                        AZURE POLYBASE

#######################################################################################################################################################################

               ----------------------------------------------------------------     **********        -------------------------------------------------------------
CREATE MASTER KEY     

  * The database master key is a symmetric key used to protect the private keys of certificates and asymmetric keys that are present in the database. When it is created
    the master key is encrypted by using the AES_256 algorithm and a user-supplied password."

               ----------------------------------------------------------------     **********        -------------------------------------------------------------

CREATE DATABASE SCOPED CREDENTIAL <credential-scope-name>
WITH  
   IDENTITY = '<username>',
   SECRET  = '<secret-key-of-source-account>'

  * CREATE DATABASE SCOPED CREDENTIAL: This statement is used to create a database-scoped credential. A database-scoped credential is used to store and manage the
                                       authentication information required to access external data sources or services.

  * IDENTITY: The IDENTITY parameter specifies the identity or username associated with the credential, you can replace it with the actual identity or username required
              for authentication.
  * SECRET: The SECRET parameter is used to specify the secret or password associated with the credential.
 
               ----------------------------------------------------------------     **********        -------------------------------------------------------------


CREATE EXTERNAL DATA SOURCE <data-source-name>
WITH
(
    TYPE = hadoop,
    LOCATION = <location-of-the-data-source> , 
    CREDENTIAL = <name-of-the-scoped-credential>
)
 
  * CREATE EXTERNAL DATA SOURCE: This statement is used to create an external data source, which represents a connection to an external data location or service
                                 <data-source-name>: You need to replace <data-source-name> with the desired name for your external data source. Choose a name that
                                 represents the data source you are connecting to.

  * TYPE:     The TYPE parameter specifies the type of the external data source. In this case, it is set to "hadoop" which indicates that the data source is based on
              Hadoop. However, depending on your specific scenario, you may need to use a different type such as "SQL Server", "Oracle", "Azure Blob Storage", or others,
              depending on the data source you are connecting to.

  * LOCATION: The LOCATION parameter specifies the location of the data source. You need to replace <location-of-the-data-source> with the actual address or location
              of your data source.

  * CREDENTIAL: The CREDENTIAL parameter specifies the name of the database-scoped credential that will be used for authentication when accessing the external data
                source. You need to replace <name-of-the-scoped-credential> with the actual name of the credential you created earlier using the
                "CREATE DATABASE SCOPED CREDENTIAL" statement as specified above.


               ----------------------------------------------------------------     **********        -------------------------------------------------------------



CREATE EXTERNAL FILE FORMAT <name-of-file-format>
WITH
(
    FORMAT_TYPE = <format-typr>,
    FORMAT_OPTIONS  (
         FIELD_TERMINATOR = <delimitter>,
         FIRST_ROW = <specify-the-number-of-ur-firstrow>
    )
);

  * CREATE EXTERNAL FILE FORMAT: This statement is used to create an external file format, which defines the structure and properties of the data files that you will be
                                 working with in Azure Synapse PolyBase or this specifies the structure and properties of the source data file that you will be
                                 working with in Azure Synapse PolyBase

  * <name-of-file-format>: You need to replace <name-of-file-format> with the desired name for your file format. Choose a name that represents the format and
                           characteristics of your data files.

  * FORMAT_TYPE: The FORMAT_TYPE parameter specifies the type of the external file format. You need to replace <format-type> with the actual format type you want to
                 use. Some common format types include "DELIMITEDTEXT", "CSV", "PARQUET", "AVRO", etc., depending on the file format you are working with.

  * FORMAT_OPTIONS: The FORMAT_OPTIONS section contains additional options and settings for the file format. It allows you to define various properties of the format.
        
       # FIELD_TERMINATOR: The FIELD_TERMINATOR option specifies the character or string used to delimit fields in the data file. You need to replace <delimiter> with
                           the actual delimiter used in your data file, such as a comma (,), tab (\t), pipe (|), etc.

       # FIRST_ROW: The FIRST_ROW option specifies the number of the first row that contains the actual data in the file. You need to replace 
                    <specify-the-number-of-ur-firstrow> with the appropriate row number. This is useful if your file includes header rows or metadata that should be
                    skipped during processing.



               ----------------------------------------------------------------     **********        -------------------------------------------------------------



CREATE EXTERNAL TABLE <table_name>
(
    <col-name> <datatype> <constraint>,
    <col-name> <datatype> <constraint>,
      :
      :
    <col-name> <datatype> <constraint>
)
WITH
(
    LOCATION = <location-of-table>,
    DATA_SOURCE = <external-data-source-name>,
    FILE_FORMAT = <external-file-format-name>
)

  * CREATE EXTERNAL TABLE: This statement is used to create an external table, which represents the structure and schema of the data stored in external data sources.
    
       # <table_name>: You need to replace <table_name> with the desired name for your external table. Choose a name that reflects the purpose or content of the table.
         
       # <col-name> <datatype> <constraint>: This section defines the columns of the external table. You need to replace <col-name>, <datatype>, and <constraint> with
                                             the actual column names, data types, and constraints according to your data schema.

       # LOCATION: The LOCATION parameter specifies the location of the external table. You need to replace <location-of-table> with the actual address or location
                   where the data files for this table are stored in the external data source.
 
       # DATA_SOURCE: The DATA_SOURCE parameter specifies the name of the external data source previously created using the CREATE EXTERNAL DATA SOURCE statement. You
                      need to replace <external-data-source-name> with the actual name of the external data source associated with this table.

       # FILE_FORMAT: The FILE_FORMAT parameter specifies the name of the external file format previously created using the CREATE EXTERNAL FILE FORMAT statement. You
                      need to replace <external-file-format-name> with the actual name of the external file format associated with this table.

  * By creating this external table, you define the structure, columns, and location of the data stored in the external data source. This allows you to query and access
    the data in the external files as if it were a regular table within Azure Synapse. However, it's important to note that the data remains stored in the external
    location and is accessed on-demand during query execution.

                ----------------------------- -----------------------------------     **********     ------------------------------------------------------------

SELECT TOP 10 * from polydemo

  * Run the above query to check weather the data transformation has completed successfully or not

                ----------------------------------------------------------------      **********     ------------------------------------------------------------


########################################################################################################################################################################

                                                                           AZURE DATABRICKS                                                                            

########################################################################################################################################################################


---------------------------------------------------------   storing the secrets inside a scope in databricks ------------------------------------------------------------
  
 ** Secrets in databricks are stored in a database and can be accessed in notebooks without hardcoding the values.
     
    The process of storing the secrets inside database is done through databricks cli and commands are listed below
    
    ! Install the Databricks CLI: pip3 install databricks-cli
    
    ! Verify the installation   : databricks --version

    ! Configure the databricks  : databricks configure token  ## here it will ask for url and access token
    
    ! Create a scope            : databricks secrets create-scope --scope <scope-name>
    
    ! To list the scopes        : databricks secrets list-scopes
    
    ! To add a secret to scope  : databricks secrets put --scope <scope-name> --key <name-of-secret> ## it will open file, store ur secret value in it
   
    ! To list all secrets stored : databricks  secrets list --scope <scope-name>
    
    ! To list the users who can
      access the scope          : databricks secrets list-acls --scope <scope-name>
      
    ! To add user to access 
      secret in a scope        : databricks secrets put-acls --scope <scope-name> --principal <azure-ad-username> --permission <READ>
                                 ## you should have certain permission to perform the operation
                                   
 

    These secrets stored inside database in azuere databricks can be managed through azure portal through key-vaults
    for more and clear info  --------------> https://www.youtube.com/watch?v=9VzBS4OiP_A

    

------------------------------------------------------------ To mount data from storage account to databricks ----------------------------------------------------------

# Mount Azure Blob storage to a specific location in Databricks file system
storage_account_name = "<your_storage_account_name>"
storage_account_access_key = "<your_storage_account_access_key>"
container_name = "<your_container_name>"
mount_point = "/mnt/<folder-name-to-mount-data>"

dbutils.fs.mount(
  source=f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net",
  mount_point=mount_point,
  extra_configs={
    f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net": storage_account_access_key
  }
)

# Read data from mounted Azure Blob storage
file_path = f"{mount_point}/<source-file-name>"
df = spark.read.format("csv").option("header", "true").load(file_path)

# Perform data processing or analysis
# For example, display the data
df.show()

# Unmount Azure Blob storage from Databricks file system
dbutils.fs.unmount(mount_point)

---------------------------------------------------------- mount data from dbfs to blob storage -----------------------------------------------------

result.write.format("csv").mode("overwrite").
      option("fs.azure.account.key.<storage_account_name>.blob.core.windows.net", "<storage_account_access_key>").
      save("wasbs://<container_name>@<storage_account_name>.blob.core.windows.net/<destinamtio-file-name>")

---------------------------------------------   to create a temporary view of the mounted csv file ---------------------------------------------------

%sql
CREATE OR REPLACE TEMPORARY VIEW <temp-table-name>
USING csv
OPTIONS (
  path "/mnt/<source-file-path>",
  header "true"
);

----------------------------------------------       extracting a table out of temporary view  -------------------------------------------------------

CREATE TABLE <table-name>
AS
SELECT * FROM <temporary-table-name> WHERE <condition>

#### how to store output of a query to variable 

result = spark.sql("select count(*) from <table-name>")
result.show() ## to display the data


#######################################################################################################################################################

                                                               Azure Monitor Api

#######################################################################################################################################################

Q) Write a python code that interacts with Azure Monitor APIs and pulls the last one month metrics(all metrics) of an AAS. And send the response over
the mail using SMTP in tabular form.

 Retrieve metric definitions, dimension values, and metric values using the Azure Monitor API and use the data in your applications, or store in a 
 database for analysis. You can also list alert rules and view activity logs using the Azure Monitor API.
 
 To retrive metrics azure monitor api u should have token first, to generate token follow the steps:

step-1: first register a app in azure active directory 
        https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/register-app-for-token?tabs=portal
        
                             
step-2: You have to give permission for the application that u have created above to retrive the metrics.
        # To give permission select the resource that u want to retrive the metrics from from "all resources" in azure portal and follow the
        documentation
        https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-store-custom-rest-api

Now u can generate the token and access the metrics from the selected resources


----------------------------------------------------------------- python code to fetch metrics ------------------------------------------------------

import requests,json
import os,smtplib,ssl
from re import findall
from email.message import EmailMessage

Subscription=os.getenv('SUBSCRIPTION')
ClintId=os.getenv('CLINTID')
TenantId=os.getenv('TENANTID')
Value=os.getenv('VALUE')
SecretId=os.getenv('SECRETID')
resouce_provider_namespace="Microsoft.AnalysisServices"
resource_type="servers"
aas_name = "sachinanalytics"
api_version="2018-01-01"
resource_group="sorryimlate"


def get_token():
  url=f"https://login.microsoftonline.com/{TenantId}/oauth2/token"
  headers={'Content-Type':'application/x-www-form-urlencoded'}
  data={
    "grant_type":'client_credentials',
    'client_id':f'{ClintId}',
    'client_secret':f'{Value}',
    'resource':'https://management.azure.com'
  }
  response = requests.post(url, headers=headers, data=data)
  return response


responce=get_token()
token = responce.json()["access_token"]

with open("result.txt",'w') as file:
    file.write("Metric name    |    Timestamp            |   memory_usage  \n")

metrics=['qpu_metric', 'memory_metric', 'private_bytes_metric', 'virtual_bytes_metric', 'TotalConnectionRequests', 'SuccessfullConnectionsPerSec',
'TotalConnectionFailures', 'CurrentUserSessions', 'QueryPoolBusyThreads', 'CommandPoolJobQueueLength', 'ProcessingPoolJobQueueLength',
'CurrentConnections', 'CleanerCurrentPrice', 'CleanerMemoryShrinkable', 'CleanerMemoryNonshrinkable', 'MemoryUsage', 'MemoryLimitHard',
'MemoryLimitHigh', 'MemoryLimitLow', 'MemoryLimitVertiPaq', 'Quota', 'QuotaBlocked', 'VertiPaqNonpaged', 'VertiPaqPaged', 'RowsReadPerSec', 
'RowsConvertedPerSec', 'RowsWrittenPerSec', 'CommandPoolBusyThreads', 'CommandPoolIdleThreads', 'LongParsingBusyThreads', 'LongParsingIdleThreads', 
'LongParsingJobQueueLength', 'ProcessingPoolBusyIOJobThreads', 'ProcessingPoolBusyNonIOThreads', 'ProcessingPoolIOJobQueueLength',
'ProcessingPoolIdleIOJobThreads', 'ProcessingPoolIdleNonIOThreads', 'QueryPoolIdleThreads', 'QueryPoolJobQueueLength', 'ShortParsingBusyThreads',
'ShortParsingIdleThreads', 'ShortParsingJobQueueLength', 'memory_thrashing_metric', 'mashup_engine_qpu_metric', 'mashup_engine_memory_metric', 
'mashup_engine_private_bytes_metric', 'mashup_engine_virtual_bytes_metric']

for metric in metrics:
  
  
  dimension_url=f'https://management.azure.com/subscriptions/{Subscription}/resourceGroups/{resource_group}/providers/{resouce_provider_namespace}/
                  {resource_type}/{aas_name}/providers/microsoft.insights/metrics?api-version=2019-07-01&metricnames={metric}&
                  timespan=2023-06-21T10:30:00Z/2023-06-22T11:00:00Z&aggregation=Total&interval=PT1M'
  headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
    "Host": "management.azure.com"
     }
  dimensions=requests.get(dimension_url,headers=headers)
  metric_dimensions=dimensions.json()['value'][0]['timeseries'][0]['data']

  with open("result.txt",'a') as file:

    for item in metric_dimensions:
        if 'total' in item.keys():
            file.write(f"{metric} | {item['timeStamp']} | {item['total']}\n")
        else:
            file.write(f"{metric} | {item['timeStamp']} |      NA   \n")

From='sachin.hs@sigmoidanalytics.com'
To='sachinshadadi@gmail.com'
password=os.getenv('SMTPPASSWD')

em=EmailMessage()
em['From']='sachin.hs@sigmoidanalytics.com'
em['To']='sachinshadadi@gmail.com'
em['Subject']="monthley metrics"

with open('result.txt') as file:
  email=file.read()
  em.set_content(email)   

context=ssl.create_default_context()
with smtplib.SMTP_SSL('smtp.gmail.com',465,context=context) as smtp:
    smtp.login(From,password)
    smtp.sendmail(From,To,em.as_string())


-------------------------------------------------------------------------------------------------------------------------------------------------------

#######################################################################################################################################################

                                                                     Cluster policy creation 

#######################################################################################################################################################

Q)Create a cluster policy to enforce below limitations -
1. No. of worker - min - 1 max - 2
2. Only allowed runtime  - 7.3.x-scala2.12
3. Autotermination and spot instance must be selected
4. A user can only launch one cluster
5.  Only allowed node types are - Standard_DS3_v2 and Standard_D3_v2
6. tag (team => devops) is a must
7. DBU hour must not increase 100
Write a python code to programmatically assign the above-created policy to  adb

Note - Upload both the files i.e. JSON and python files.

-------------------------------------------------------------  .json file------------------------------------------------------------------------------
{
  "name": "ExamplePolicy",
  "definition": {
        "cluster_name": "my-cluster",
        "num_workers": {
          "type": "range",
          "minValue": 1,
          "maxValue" : 2
       },
       "spark_version": {
          "type": "fixed",
          "defaultValue": "7.3.x-scala2.12"
       },
       "maxClustersPerUser": {
        "type": "fixed",
        "value": 1
       },
     "autotermination_minutes": {
          "type": "fixed",
          "value": 30,
          "hidden": true
       },
       "node_type_id": {
        "type": "allowlist",
        "values": [
          "Standard_DS3_v2",
          "Standard_D3_v2"
        ],
        "defaultValue": "Standard_DS3_v2"
      },
      "custom_tags.team": {
        "type": "fixed",
        "value": "devops"
      },
      "dbus_per_hour": {
        "type": "range",
        "maxValue": 100
      },
      "spot": {
        "enabled": true
      }
  }
}

-------------------------------------------------------------------------------- .py file ------------------------------------------------------------

import json,requests

Host='https://community.cloud.databricks.com/?o=3327665337570993#'
host_token='host_token'

with open ("cluster.json","r") as file:
    cluster=json.load(file)
    #print(type(cluster))

data=(json.dumps(cluster))
#print((data))

headers={
    "Authorization': f'Bearer {host_token}"
}

requests.post(f"{Host}/api/2.0/policies/clusters/create",json=data)

-----------------------------------------------------------  AZURE KUBERNETES SERVICE ----------------------------------------------

* To oen the shell terminal          ------> shell.azure.com
* To install azure cli in local      ------> az aks install-cli
* To generate credentials 4 login    ------> az aks get-credentials --resource-group <rg-name> --name <cluster-name>
                                             # it will create a kubeconfig file and store the credentials in home/<usename>/.kube/config
* To get all nodes                   ------> kubectl get nodes -o [ wide | yaml ] #yaml to get info in yaml format
* To get namespaces                  ------> kubectl get namespaces (or) kubectl get ns
* To deploy app on cluster           ------> kubectl run <name> --image <username>/imagename:<tag>
* To deploy an image                 ------> kubectl apply -f <file-name>
* To get all the workloads           ------> kubectl get pods --all-namespaces
* To get all namespaces              ------> kubectl get all --all-namespaces
* To get the pods                    ------> kubectl get pods -o wide
* To get addition/discribe info      ------> kubectl describe pod <podname>
* To get the service                 ------> kubectl get svc
* To get the url                     ------> minikube service <service-name> --url
* To delete the pod                  ------> kubectl delete pod <pod-name>
* To delete service                  ------> kubectl delete service/<service-name>
* To delete replicaset               ------> kubectl delete rs/<replica-set-name>
* To create a service directley      ------> kubectl expose pod <pod-name>  --type=LoadBalancer --port=<port> --name=<service-name>
* To get the logs of pod             ------> kubectl logs <pod-name>
* To get the stream logs             ------> kubectl logs -f <pod-name>
* To connect to container in pod     ------> kubectl exec -it <pod-name> -- /bin/bash
* To interact without entering pod   ------> kubectl exec -it <pod-name> -- <command> ##for eg : ls, pwd etc...

--------------------------------------------- replica set ----------------------------------------------------

* To get the replicaset info         ------> kubectl get [ replicaset | rs ]
* To descrive replicaset             ------> kubectl describe [ rs | replicaset ] <replicaset-name>
* Expose replicaset as a service     ------> kubectl expose rs <replica-name> --type=LoadBalancer --port=80 --target-port=8080 --name=<service name>
* To increse the replicas through yaml
  file and to replace those changes  ------> kubectl replace -f <replics-file-name>

------------------------------------------------- deployment -------------------------------------------------------
* To deploy an image as a deployment ------> kubectl create deployment <deployment-name> --image=<repo-name>/<imagename>:<tag>
* To scale the replicas of deployment -----> kubectl scale --replicas=<number> deployment/<deployment-name>
* To get info of deployment           -----> kubectl get deploy <deployment> -o [ wide | yaml ]

---------------------------------------------- to rollout to different version -----------------------------------------------------------

* To change the version of image      -----> kubectl set image deployment/<deployment-name> <container-name>=<new-image-name>  --record=true
                                               # to get the conainer name try " kubectl get deploy <deployment> -o yaml" and look for spec.containers.name
* To edit deployment through edit 
   deployment file                    -----> kubectl edit deployment/<deployment-name> --record=true
* To get previous version changes made ----> kubectl rollout history deployment/<deployment-name>  # gives u the history of previous roll outs

--------------------------------------------------- to rollback to previous version of deployment ------------------------------------------------

* To check the roll out history         -----> kubectl rollout history deployment/<deployment-name>
* To verify the changes made            -----> kubectl rollout history deployment/<deployment-name> --revision=<number> 
* To rollback to previous version       -----> kubectl rollout undo deployment/<deployment-name>
* To rollback to specific version       -----> kubectl rollout undo deployment/<deployment-name> --to-revision=<number>

---------------------------------------------------------- pause and resume deployments  ------------------------------------------------------

* To pause the deployment              -----> kubectl rollout pause deployment/<deployment-name>
* To make any updates or set resources -----> kubectl set resource deployment/<deployment-name> -c=<container0-name> --limits=cpu=20m,memory=30Mi
* To resume the deployment             -----> kubectl rollout resume deployment/<deployment-name>

----------------------------------------------------- services ----------------------------------------------------------------------

# Available services                        -----> ClusterIP, NodePort, LoadBalancer, Ingress, externalName
# clusterip                                 -----> used for communcation b/w app inside cluster (front and backend)
# NodePort                                  -----> used to access app outside the k8s cluster 
# LoadBalancer                              -----> Primarily to cloud providers for integrate with load balancer
# Ingress                                   -----> An advanced load balancer provide content path based routing (ssl,ssl redirect--etc)
# externalName                              -----> To access external hosted apps in k8s cluster (eg; access aws RDS DATABASE endpoint by an app in k8s cluster)

------------------------------------------------  yaml basics -------------------------------------------------------------------------------

name: sachin                     -------->  {"name":"sachin"}
person:
    name: sachin                 
    age: 24                      --------> {"person":{"name":"sachin", "age":24}}
    hobies:
      - cycling
      - coocking                 --------> {"person":{"name":"sachin", "age":24, "hobies":["cycling", "coocking"]}}
    friends:
      - name: souvik
        age: 24
      - name: ankit              --------> {"person":{"name":"sachin", "age":24, "hobies":["cycling", "coocking"], "friends":[{"name":"souvik", "age":24}, {"name":"ankit", "age": 24}]}}
        age: 24

---  ##  yaml seperator

# second object 
name: ankit                     -------->  {"name":"ankit"}
person:
    name: ankit                
    age: 24                      --------> {"person":{"name":"ankit", "age":24}}
    hobies:
      - cycling
      - coocking                 --------> {"person":{"name":"ankit", "age":24, "hobies":["cycling", "coocking"]}}
    friends:
      - name: souvik
        age: 24
      - name: ankit              --------> {"person":{"name":"ankit", "age":24, "hobies":["cycling", "coocking"], "friends":[{"name":"souvik", "age":24}, {"name":"ankit", "age": 24}]}}
        age: 24

      