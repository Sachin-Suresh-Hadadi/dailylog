 ################################                                Basic linux commands                    ################################################
 
* To check the configuration              -----------> lsb_release -a (or) cat /etc/*release (or) lsb_release -dc
* For getting detailed information about 
   how to use various commands and        -----------> man [options] [command/function]
                                                                # options --> [-k,-f] -k to search by keyword and -f to  Display one-line description of a command.
   utilities in a Linux system.                                 # Eg ; 1) man ls  ; 2) man -k "file permissions"  ; 3) man -f tar
   
* To get top most content of a file       -----------> head -n 5 sample.txt (or) head -c 100 file.txt
                                                                # -n : to display first five lines
                                                                # -c : to display the first 100 bytes of file.txt

* To get last content of a file          ------------> tail -n 10 access.log (or) tail -f error.log
                                                                # -f : To monitor log file in real time

* To get content of file page after page   ----------> more <file-name>
* To get content of file line after line   ----------> less [options] <filename>  
                                                                # options
                                                                  -N: Show line numbers in the left margin.
                                                                  -i: Ignore case when searching.
                                                                   q: Quit less and return to the terminal.
                                                                   
* To check top content of compressed file  ----------> zcat <filename.gz> | head -n <number-of-lines-to-display>   (or) zless <filename.gz>
* To split the large file into smaller files                                        
  based on bytes                             --------> split -b   [1K | 1M | 1G ] -d -a 1 <input_file> <output_prefix>
                                                                # -d : to command that the child files must have a prefix-names as numbers
                                                                # -a : to specify the lenth of prefix 
* To split the large file into smaller files                                        
  based on LINES                             --------> split -l <number> -d -a 1 <input_file> <output_prefix>
                                                                # it will create one child file for every n number of lines with prefix 1,2,3,4----

* screen
   ! To create a screen                      --------> screen -S <screen-name>
   ! To list the all screen                  --------> screen -ls
   ! To detach from screen                   --------> ctrl+a d
   ! To re-attach to screen                  --------> screen -r <screen-name>
   ! To delete a screen                      --------> screen -S <screen-name> -X quit

* jq
   ! 
* lsof
   
* mtr

* shread

* md5sum

* sysctl -p

* lsblk

* lvm 


 #####################################################   docker instalation using conventional script #################################################

link for documentation     ----> https://docs.docker.com/engine/install/ubuntu/#install-using-the-convenience-script 

* To login to docker hub   ----> docker login
* To build a image         ----> docker build . -f <docker-file-name> -t <docker-username>/<app-name>
* To push to docker hub    ----> docker push <docker-username>/<app-name>
* To see runnning containers --> docker ps
* To list all stoped &running -> docker ps -a
* To get container id      ----> docker ps -a -q
* To start container       ----> docker start <cont-id>
* To connect to containe   ----> docker exec -it <cont-name> /bin/sh
* To stop container        ----> docker stop <cont-name>
* To remove container      ----> docker rm   <cont-name>
* To remove image          ----> docker rmi  <image-name> or docker rmi -f <image-name> ## -f for remove forcefully
* To add tag to image      ----> docker tag  <docker-hub-id>/<image-name>:<tag-name>
* To push newly tagged image --> docker push <docker-hub-id>/<image-name>:<tag-name>
* To display livestats     ----> docker stats


* django project 2 docker  ----> https://www.youtube.com/watch?v=W5Ov0H7E_o4
######################### ##  enable ssh-authentcation ###############
* To enable ssh-authentication , first create an ssh keys 
  $ ssh-keygen -t rsa

* After creating successfully, from user directory, open the .ssh folder
  $ cd .ssh
  
* lookout for "id_rsa.pub" file and copy the contents of the file

* Login to github through UI and under settings clickk for SSH AND GPG keys on left hand bar

* Click on New SSH key , provide the title and paste the copied content in the space provided under key and click on Add SSH key


########################### basi-git-commands #######################

* To install git               ---->  sudo apt install git
* To ckeck the version         ---->  git version
* To configure username        ---->  git config --global user.name "git_user_name"
* To configure email           ---->  git config --global user.email "git_user_mail"
* To clone repository          ---->  git clone <git--url>
* To add a remote repo         ---->  git remote add <remote-repo-name> <url-of-source-repo>
* To check status              ---->  git status
* To add to staginng area      ---->  git add <file name>  (or)  git add .      # "." to add all the files in taging area
* To add only modified files 
  leaving untracked files      ---->  git add -u
* To stage all files           ---->  git add -A
* To commit the file/change    ---->  git commit -m "<commit-message>"
* To push to the remote repo   ---->  git push <remote-repo-name> <name-of-branch-to-push>
* To start fresh git repo
  in local/ initialise         ---->  git init 
* To configure username        ---->  git config --global user.name "git_user_name"
* To configure email           ---->  git config --global user.name "git_user_mail"
* To list user                 ---->  git config --global --list    
* To get all tracked files     ---->  git ls-files
* To get logs of commits       ---->  git log (or) git log --abbrev-commit (or) git log --oneline --graph --decorate --all
* To get a logs in between 2 
  commits                      ----> git log <from_commit_id>...<to-commit-id>
* To get commits happend in 
  last n days                  ----> git log --since="<n> days ago"  # n= no of days
* To get logs of a file        ----> git log -- <file_name>
* To get log of renamed        ----> git log -- follow -- <file-path>
* To show about specific commit ---> git show <commit-id>
* To edit the commit message   ----> git commit --amend
* To change url from https to ssh -> git remote set-url origin <--link/url-->
* To rename the repo as in remote 
  repo                          ---> git remote set-url origin <--link/url-->
 
################## git workflow (push,pull) ####################

* To pull from remote repo     ----> git pull <remote-repo-name> <name-of-branch-to-push>
* To push to the remote repo   ----> git push <remote-repo-name> <name-of-branch-to-push>

#################  tracking files    ##################

* To add and commit the 
  tracked file                 ----> git commit -am "<your-message>"   # trackerd file : These are the files previously commited to git

############ backing out changes (undo,rename,move,delete)    ###################

* To backout changes added to 
  staging area                 ----> git reset HEAD <name-of-file>    # To unstage from staging area
* To undo the changes made     ----> git checkout -- <file-name-to-undo-changes>  # this will undo the changes that have been made to a file after the last commit
* To rename file using git     ----> git mv <source_file> <new-file-name>
* To move file using git       ----> git mv <source_file_path> <destination_file_path> 
* To remove tracked file       ----> git rm <file_to_remove>  # the specified file move from committed to staging area and commit to remove 
* To undo delete staged file 
  that is in staging area       ---> git reset HEAD <file_name> # the file will be added to git repo but will not be present in working directory to get it back in wd
* To undo deleted file to 
  working directory             ---> git checkout <filename>
* To add deletetion done  
  directley on working directory --> git add -A # If a staged file is removed/deleted directley from working directory, this will be in staging area, commit to 
                                                  delete permenentley
       
#############  how to set up alias in git ###############

* To create alias              ----> git config --global alias.<alias-name> "<operation-to-perform>" 
                                     ## eg git config --global alias.history "log --all --oneline --graph --decorate"
* To update alias              ----> nano ~/.gitconfig

#############  comparing differences   #############

* To get diff bw staging area 
  and working directory        ----> git diff (or) git diff -- <file-name> # to get the difference of a specific file
* To get diff bw last commit
  and working directory        ----> git diff HEAD
* To get diff bw staged and
  and last commit              ----> git diff --staged HEAD
* To get diff between committs ----> git diff <commit-id1> <comit-id2>
* To compare b/w last 2 commit ----> git diff HEAD HEAD^
* To get diff b/w branches     ----> git diff master origin/master

###########  branching and merging    ########################

* To list local branch         -----> git branch
* To list all branches         -----> git branch -a ## list all local and remote 
* To create a branch           -----> git branch <branch-name>-----------------------|
                                                                                     |-----> to create and switch simontaniously ==> git checkout -b <branch-name>  
* To switch branch             -----> git checkout <branch-name>---------------------|
* To renmane branch            -----> git branch -m <old_name> <new_name>
* To delete branch             -----> git branch -d <old_name> <new_name>
* To merge                     -----> git merge <name-of-the-branch-to-merge> -m "<message>" ## merge with master/main branch and see difference beform merging
* To merge without fast forword ----> git merge <name-of-the-branch-to-merge> --no-ff -m "<message>"
* Merging conflict             -----> 
 
#####################  rebeasing  ############################

* To rebasing to main/master  ------> git rebase <branch-name> # aftre branching out, if u want to get all the updates/modifications from specified branch
* To abort rebase if conflict 
  is raised                   ------> git rebase --abort
* To merge conflicts          ------> git mergetool --tool=<your_merge_tool_name>
* To continue rebase after
  solving conflicts           ------> git rebase --continue

############   stashing   ######################

* Stashing is like pausing, if we want to pause the curent working and work on something important, we can stach the process as it pauses and changes all the files
  to previous commit
* To stash/pause               -----> git stash 
* To undo stash                -----> git stash apply
* To list all stashes          -----> git stash list
* To drop the stash            -----> git stash drop
* To consider even untracked 
  files                        -----> git stash -u
* To combine both apply & drop -----> git stash pop
* To save stash with message   -----> git stash save "<stash_message>"
* To get info of specific stash ----> git stash show stash@{index_num}  # you will get index_num if you do "git stash list"\
* To apply to a specific stash  ----> git stash apply stash@{index_num}
* To drop a specific stash     -----> git stash drop stash@{index_num}
* To delete all stashes        -----> git stash clear
* To switch to new branch     ------> git stash branch <branch_name> # it will create & switch 2 new specified branch and drops the stash 

###################  tagging  ############################

* To create a tag             ------> git tag <tag_name> ##light-weight tag
* To list tags                ------> git tag --list
* To get info of tag          ------> git show <tag_name>
* To delete tag               ------> git tag --delete <tag-name>
* To create annotated tag     ------> git tag -a <tag-name> -m "<tag_message>" ## gives some more info than leight-weight tag
* To diplay diff b/w tags     ------> git diff <first-tag> <second-tag>
* To tagging specific commit  ------> git tag -a <tag-name> -m <tag-msg> <commit-id>
* To update tags              ------> git tag -a <tag-name> -m <message> -f <correct-commit-id>
* To push a specific tag to remote -> git push <remote-repo-name> <tag-name>
* To push all tags to remote  ------> git push <remote_repo_name> <branch_name_2_push> --tags
* To delete a tag in remote repo -->  git push <remote-repo-name> :<tag-name> ## delete tag from repo but will be present in local

############################## reset,reflog and compare branches  ##########################################

* To set head back to previous
  nth commit                   ----> git reset HEAD^(n) git reset HEAD@{n} or   ## n specifies no of commits back u want to go to
* To get history of all the 
  reset commands              -----> git reflog
* To change to a specific 
  commit using commit id      -----> git reset <commit-id> ### u will get id by "git reflog"

####################################### github actions and how to create and clone a repository through cli ###########################################

* Login to github             ------> gh auth login
* Create and clone a repo     ------> gh repo create <git-user-name>/<repo-name> --<public/private> -c  ## "-c" flag to clone the repo
* To run the workflow from cli  ----> gh workflow run <action-filename>.yml
* To list the workflows       ------> gh run list --workflow = <action-file-name>.yml  ## here you will get job id
* To get logs from running job  ----> gh run view --log --job = <job-id> ## get job id from above command


#######################################################################################################################################################################

                                                                       AZURE SQL
#######################################################################################################################################################################

  * How to create a table

CREATE EXTERNAL TABLE <table_name>
(
    <col-name> <datatype> <constraint>,
    <col-name> <datatype> <constraint>,
      :
      :
    <col-name> <datatype> <constraint>
)


  * How to insert value to table

INSERT INTO <TABLE-NAME> ( <col-name>, <col-name>) VALUES ( value1,value2)

  * How to create a table as CTAS
 
CREATE TABLE <destinatio-tablename>
AS
SELECT * FROM <SOURCE-TABLE-NAME> WHERE <condition>



#######################################################################################################################################################################

                                                                        AZURE POLYBASE

#######################################################################################################################################################################

               ----------------------------------------------------------------     **********        -------------------------------------------------------------
CREATE MASTER KEY     

  * The database master key is a symmetric key used to protect the private keys of certificates and asymmetric keys that are present in the database. When it is created
    the master key is encrypted by using the AES_256 algorithm and a user-supplied password."

               ----------------------------------------------------------------     **********        -------------------------------------------------------------

CREATE DATABASE SCOPED CREDENTIAL <credential-scope-name>
WITH  
   IDENTITY = '<username>',
   SECRET  = '<secret-key-of-source-account>'

  * CREATE DATABASE SCOPED CREDENTIAL: This statement is used to create a database-scoped credential. A database-scoped credential is used to store and manage the
                                       authentication information required to access external data sources or services.

  * IDENTITY: The IDENTITY parameter specifies the identity or username associated with the credential, you can replace it with the actual identity or username required
              for authentication.
  * SECRET: The SECRET parameter is used to specify the secret or password associated with the credential.
 
               ----------------------------------------------------------------     **********        -------------------------------------------------------------


CREATE EXTERNAL DATA SOURCE <data-source-name>
WITH
(
    TYPE = hadoop,
    LOCATION = <location-of-the-data-source> , 
    CREDENTIAL = <name-of-the-scoped-credential>
)
 
  * CREATE EXTERNAL DATA SOURCE: This statement is used to create an external data source, which represents a connection to an external data location or service
                                 <data-source-name>: You need to replace <data-source-name> with the desired name for your external data source. Choose a name that
                                 represents the data source you are connecting to.

  * TYPE:     The TYPE parameter specifies the type of the external data source. In this case, it is set to "hadoop" which indicates that the data source is based on
              Hadoop. However, depending on your specific scenario, you may need to use a different type such as "SQL Server", "Oracle", "Azure Blob Storage", or others,
              depending on the data source you are connecting to.

  * LOCATION: The LOCATION parameter specifies the location of the data source. You need to replace <location-of-the-data-source> with the actual address or location
              of your data source.

  * CREDENTIAL: The CREDENTIAL parameter specifies the name of the database-scoped credential that will be used for authentication when accessing the external data
                source. You need to replace <name-of-the-scoped-credential> with the actual name of the credential you created earlier using the
                "CREATE DATABASE SCOPED CREDENTIAL" statement as specified above.


               ----------------------------------------------------------------     **********        -------------------------------------------------------------



CREATE EXTERNAL FILE FORMAT <name-of-file-format>
WITH
(
    FORMAT_TYPE = <format-typr>,
    FORMAT_OPTIONS  (
         FIELD_TERMINATOR = <delimitter>,
         FIRST_ROW = <specify-the-number-of-ur-firstrow>
    )
);

  * CREATE EXTERNAL FILE FORMAT: This statement is used to create an external file format, which defines the structure and properties of the data files that you will be
                                 working with in Azure Synapse PolyBase or this specifies the structure and properties of the source data file that you will be
                                 working with in Azure Synapse PolyBase

  * <name-of-file-format>: You need to replace <name-of-file-format> with the desired name for your file format. Choose a name that represents the format and
                           characteristics of your data files.

  * FORMAT_TYPE: The FORMAT_TYPE parameter specifies the type of the external file format. You need to replace <format-type> with the actual format type you want to
                 use. Some common format types include "DELIMITEDTEXT", "CSV", "PARQUET", "AVRO", etc., depending on the file format you are working with.

  * FORMAT_OPTIONS: The FORMAT_OPTIONS section contains additional options and settings for the file format. It allows you to define various properties of the format.
        
       # FIELD_TERMINATOR: The FIELD_TERMINATOR option specifies the character or string used to delimit fields in the data file. You need to replace <delimiter> with
                           the actual delimiter used in your data file, such as a comma (,), tab (\t), pipe (|), etc.

       # FIRST_ROW: The FIRST_ROW option specifies the number of the first row that contains the actual data in the file. You need to replace 
                    <specify-the-number-of-ur-firstrow> with the appropriate row number. This is useful if your file includes header rows or metadata that should be
                    skipped during processing.



               ----------------------------------------------------------------     **********        -------------------------------------------------------------



CREATE EXTERNAL TABLE <table_name>
(
    <col-name> <datatype> <constraint>,
    <col-name> <datatype> <constraint>,
      :
      :
    <col-name> <datatype> <constraint>
)
WITH
(
    LOCATION = <location-of-table>,
    DATA_SOURCE = <external-data-source-name>,
    FILE_FORMAT = <external-file-format-name>
)

  * CREATE EXTERNAL TABLE: This statement is used to create an external table, which represents the structure and schema of the data stored in external data sources.
    
       # <table_name>: You need to replace <table_name> with the desired name for your external table. Choose a name that reflects the purpose or content of the table.
         
       # <col-name> <datatype> <constraint>: This section defines the columns of the external table. You need to replace <col-name>, <datatype>, and <constraint> with
                                             the actual column names, data types, and constraints according to your data schema.

       # LOCATION: The LOCATION parameter specifies the location of the external table. You need to replace <location-of-table> with the actual address or location
                   where the data files for this table are stored in the external data source.
 
       # DATA_SOURCE: The DATA_SOURCE parameter specifies the name of the external data source previously created using the CREATE EXTERNAL DATA SOURCE statement. You
                      need to replace <external-data-source-name> with the actual name of the external data source associated with this table.

       # FILE_FORMAT: The FILE_FORMAT parameter specifies the name of the external file format previously created using the CREATE EXTERNAL FILE FORMAT statement. You
                      need to replace <external-file-format-name> with the actual name of the external file format associated with this table.

  * By creating this external table, you define the structure, columns, and location of the data stored in the external data source. This allows you to query and access
    the data in the external files as if it were a regular table within Azure Synapse. However, it's important to note that the data remains stored in the external
    location and is accessed on-demand during query execution.

                ----------------------------- -----------------------------------     **********     ------------------------------------------------------------

SELECT TOP 10 * from polydemo

  * Run the above query to check weather the data transformation has completed successfully or not

                ----------------------------------------------------------------      **********     ------------------------------------------------------------


########################################################################################################################################################################

                                                                           AZURE DATABRICKS                                                                            

########################################################################################################################################################################


---------------------------------------------------------   storing the secrets inside a scope in databricks ------------------------------------------------------------
  
 ** Secrets in databricks are stored in a database and can be accessed in notebooks without hardcoding the values.
     
    The process of storing the secrets inside database is done through databricks cli and commands are listed below
    
    ! Install the Databricks CLI: pip3 install databricks-cli
    
    ! Verify the installation   : databricks --version

    ! Configure the databricks  : databricks configure token  ## here it will ask for url and access token
    
    ! Create a scope            : databricks secrets create-scope --scope <scope-name>
    
    ! To list the scopes        : databricks secrets list-scopes
    
    ! To add a secret to scope  : databricks secrets put --scope <scope-name> --key <name-of-secret> ## it will open file, store ur secret value in it
   
    ! To list all secrets stored : databricks  secrets list --scope <scope-name>
    
    ! To list the users who can
      access the scope          : databricks secrets list-acls --scope <scope-name>
      
    ! To add user to access 
      secret in a scope        : databricks secrets put-acls --scope <scope-name> --principal <azure-ad-username> --permission <READ>
                                 ## you should have certain permission to perform the operation
                                   
 

    These secrets stored inside database in azuere databricks can be managed through azure portal through key-vaults
    for more and clear info  --------------> https://www.youtube.com/watch?v=9VzBS4OiP_A

    

------------------------------------------------------------ To mount data from storage account to databricks ----------------------------------------------------------

# Mount Azure Blob storage to a specific location in Databricks file system
storage_account_name = "<your_storage_account_name>"
storage_account_access_key = "<your_storage_account_access_key>"
container_name = "<your_container_name>"
mount_point = "/mnt/<folder-name-to-mount-data>"

dbutils.fs.mount(
  source=f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net",
  mount_point=mount_point,
  extra_configs={
    f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net": storage_account_access_key
  }
)

# Read data from mounted Azure Blob storage
file_path = f"{mount_point}/<source-file-name>"
df = spark.read.format("csv").option("header", "true").load(file_path)

# Perform data processing or analysis
# For example, display the data
df.show()

# Unmount Azure Blob storage from Databricks file system
dbutils.fs.unmount(mount_point)

---------------------------------------------------------- mount data from dbfs to blob storage -----------------------------------------------------

result.write.format("csv").mode("overwrite").
      option("fs.azure.account.key.<storage_account_name>.blob.core.windows.net", "<storage_account_access_key>").
      save("wasbs://<container_name>@<storage_account_name>.blob.core.windows.net/<destinamtio-file-name>")

---------------------------------------------   to create a temporary view of the mounted csv file ---------------------------------------------------

%sql
CREATE OR REPLACE TEMPORARY VIEW <temp-table-name>
USING csv
OPTIONS (
  path "/mnt/<source-file-path>",
  header "true"
);

----------------------------------------------       extracting a table out of temporary view  -------------------------------------------------------

CREATE TABLE <table-name>
AS
SELECT * FROM <temporary-table-name> WHERE <condition>

#### how to store output of a query to variable 

result = spark.sql("select count(*) from <table-name>")
result.show() ## to display the data


#######################################################################################################################################################

                                                               Azure Monitor Api

#######################################################################################################################################################

Q) Write a python code that interacts with Azure Monitor APIs and pulls the last one month metrics(all metrics) of an AAS. And send the response over
the mail using SMTP in tabular form.

 Retrieve metric definitions, dimension values, and metric values using the Azure Monitor API and use the data in your applications, or store in a 
 database for analysis. You can also list alert rules and view activity logs using the Azure Monitor API.
 
 To retrive metrics azure monitor api u should have token first, to generate token follow the steps:

step-1: first register a app in azure active directory 
        https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/register-app-for-token?tabs=portal
        
                             
step-2: You have to give permission for the application that u have created above to retrive the metrics.
        # To give permission select the resource that u want to retrive the metrics from from "all resources" in azure portal and follow the
        documentation
        https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-store-custom-rest-api

Now u can generate the token and access the metrics from the selected resources


----------------------------------------------------------------- python code to fetch metrics ------------------------------------------------------

import requests,json
import os,smtplib,ssl
from re import findall
from email.message import EmailMessage

Subscription=os.getenv('SUBSCRIPTION')
ClintId=os.getenv('CLINTID')
TenantId=os.getenv('TENANTID')
Value=os.getenv('VALUE')
SecretId=os.getenv('SECRETID')
resouce_provider_namespace="Microsoft.AnalysisServices"
resource_type="servers"
aas_name = "sachinanalytics"
api_version="2018-01-01"
resource_group="sorryimlate"


def get_token():
  url=f"https://login.microsoftonline.com/{TenantId}/oauth2/token"
  headers={'Content-Type':'application/x-www-form-urlencoded'}
  data={
    "grant_type":'client_credentials',
    'client_id':f'{ClintId}',
    'client_secret':f'{Value}',
    'resource':'https://management.azure.com'
  }
  response = requests.post(url, headers=headers, data=data)
  return response


responce=get_token()
token = responce.json()["access_token"]

with open("result.txt",'w') as file:
    file.write("Metric name    |    Timestamp            |   memory_usage  \n")

metrics=['qpu_metric', 'memory_metric', 'private_bytes_metric', 'virtual_bytes_metric', 'TotalConnectionRequests', 'SuccessfullConnectionsPerSec',
'TotalConnectionFailures', 'CurrentUserSessions', 'QueryPoolBusyThreads', 'CommandPoolJobQueueLength', 'ProcessingPoolJobQueueLength',
'CurrentConnections', 'CleanerCurrentPrice', 'CleanerMemoryShrinkable', 'CleanerMemoryNonshrinkable', 'MemoryUsage', 'MemoryLimitHard',
'MemoryLimitHigh', 'MemoryLimitLow', 'MemoryLimitVertiPaq', 'Quota', 'QuotaBlocked', 'VertiPaqNonpaged', 'VertiPaqPaged', 'RowsReadPerSec', 
'RowsConvertedPerSec', 'RowsWrittenPerSec', 'CommandPoolBusyThreads', 'CommandPoolIdleThreads', 'LongParsingBusyThreads', 'LongParsingIdleThreads', 
'LongParsingJobQueueLength', 'ProcessingPoolBusyIOJobThreads', 'ProcessingPoolBusyNonIOThreads', 'ProcessingPoolIOJobQueueLength',
'ProcessingPoolIdleIOJobThreads', 'ProcessingPoolIdleNonIOThreads', 'QueryPoolIdleThreads', 'QueryPoolJobQueueLength', 'ShortParsingBusyThreads',
'ShortParsingIdleThreads', 'ShortParsingJobQueueLength', 'memory_thrashing_metric', 'mashup_engine_qpu_metric', 'mashup_engine_memory_metric', 
'mashup_engine_private_bytes_metric', 'mashup_engine_virtual_bytes_metric']

for metric in metrics:
  
  
  dimension_url=f'https://management.azure.com/subscriptions/{Subscription}/resourceGroups/{resource_group}/providers/{resouce_provider_namespace}/
                  {resource_type}/{aas_name}/providers/microsoft.insights/metrics?api-version=2019-07-01&metricnames={metric}&
                  timespan=2023-06-21T10:30:00Z/2023-06-22T11:00:00Z&aggregation=Total&interval=PT1M'
  headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
    "Host": "management.azure.com"
     }
  dimensions=requests.get(dimension_url,headers=headers)
  metric_dimensions=dimensions.json()['value'][0]['timeseries'][0]['data']

  with open("result.txt",'a') as file:

    for item in metric_dimensions:
        if 'total' in item.keys():
            file.write(f"{metric} | {item['timeStamp']} | {item['total']}\n")
        else:
            file.write(f"{metric} | {item['timeStamp']} |      NA   \n")

From='sachin.hs@sigmoidanalytics.com'
To='sachinshadadi@gmail.com'
password=os.getenv('SMTPPASSWD')

em=EmailMessage()
em['From']='sachin.hs@sigmoidanalytics.com'
em['To']='sachinshadadi@gmail.com'
em['Subject']="monthley metrics"

with open('result.txt') as file:
  email=file.read()
  em.set_content(email)   

context=ssl.create_default_context()
with smtplib.SMTP_SSL('smtp.gmail.com',465,context=context) as smtp:
    smtp.login(From,password)
    smtp.sendmail(From,To,em.as_string())


-------------------------------------------------------------------------------------------------------------------------------------------------------

#######################################################################################################################################################

                                                                     Cluster policy creation 

#######################################################################################################################################################

Q)Create a cluster policy to enforce below limitations -
1. No. of worker - min - 1 max - 2
2. Only allowed runtime  - 7.3.x-scala2.12
3. Autotermination and spot instance must be selected
4. A user can only launch one cluster
5.  Only allowed node types are - Standard_DS3_v2 and Standard_D3_v2
6. tag (team => devops) is a must
7. DBU hour must not increase 100
Write a python code to programmatically assign the above-created policy to  adb

Note - Upload both the files i.e. JSON and python files.

-------------------------------------------------------------  .json file------------------------------------------------------------------------------
{
  "name": "ExamplePolicy",
  "definition": {
        "cluster_name": "my-cluster",
        "num_workers": {
          "type": "range",
          "minValue": 1,
          "maxValue" : 2
       },
       "spark_version": {
          "type": "fixed",
          "defaultValue": "7.3.x-scala2.12"
       },
       "maxClustersPerUser": {
        "type": "fixed",
        "value": 1
       },
     "autotermination_minutes": {
          "type": "fixed",
          "value": 30,
          "hidden": true
       },
       "node_type_id": {
        "type": "allowlist",
        "values": [
          "Standard_DS3_v2",
          "Standard_D3_v2"
        ],
        "defaultValue": "Standard_DS3_v2"
      },
      "custom_tags.team": {
        "type": "fixed",
        "value": "devops"
      },
      "dbus_per_hour": {
        "type": "range",
        "maxValue": 100
      },
      "spot": {
        "enabled": true
      }
  }
}

-------------------------------------------------------------------------------- .py file ------------------------------------------------------------

import json,requests

Host='https://community.cloud.databricks.com/?o=3327665337570993#'
host_token='host_token'

with open ("cluster.json","r") as file:
    cluster=json.load(file)
    #print(type(cluster))

data=(json.dumps(cluster))
#print((data))

headers={
    "Authorization': f'Bearer {host_token}"
}

requests.post(f"{Host}/api/2.0/policies/clusters/create",json=data)

-----------------------------------------------------------  AZURE KUBERNETES SERVICE ----------------------------------------------

* To oen the shell terminal          ------> shell.azure.com
* To install azure cli in local      ------> az aks install-cli
* To generate credentials 4 login    ------> az aks get-credentials --resource-group <rg-name> --name <cluster-name>
                                             # it will create a kubeconfig file and store the credentials in home/<usename>/.kube/config
* To get all nodes                   ------> kubectl get nodes -o [ wide | yaml ] #yaml to get info in yaml format
* To get namespaces                  ------> kubectl get namespaces (or) kubectl get ns
* To deploy app on cluster           ------> kubectl run <name> --image <username>/imagename:<tag>
* To deploy an image                 ------> kubectl apply -f <file-name>
* To get all the workloads           ------> kubectl get pods --all-namespaces
* To get all namespaces              ------> kubectl get all --all-namespaces
* To get the pods                    ------> kubectl get pods -o wide
* To get addition/discribe info      ------> kubectl describe pod <podname>
* To get the service                 ------> kubectl get svc
* To get the url                     ------> minikube service <service-name> --url
* To delete the pod                  ------> kubectl delete pod <pod-name>
* To delete service                  ------> kubectl delete service/<service-name>
* To delete replicaset               ------> kubectl delete rs/<replica-set-name>
* To create a service directley      ------> kubectl expose pod <pod-name>  --type=LoadBalancer --port=<port> --name=<service-name>
* To get the logs of pod             ------> kubectl logs <pod-name>
* To get the stream logs             ------> kubectl logs -f <pod-name>
* To connect to container in pod     ------> kubectl exec -it <pod-name> -- /bin/bash
* To interact without entering pod   ------> kubectl exec -it <pod-name> -- <command> ##for eg : ls, pwd etc...

--------------------------------------------- replica set ----------------------------------------------------

* To get the replicaset info         ------> kubectl get [ replicaset | rs ]
* To descrive replicaset             ------> kubectl describe [ rs | replicaset ] <replicaset-name>
* Expose replicaset as a service     ------> kubectl expose rs <replica-name> --type=LoadBalancer --port=80 --target-port=8080 --name=<service name>
* To increse the replicas through yaml
  file and to replace those changes  ------> kubectl replace -f <replics-file-name>

------------------------------------------------- deployment -------------------------------------------------------
* To deploy an image as a deployment ------> kubectl create deployment <deployment-name> --image=<repo-name>/<imagename>:<tag>
* To scale the replicas of deployment -----> kubectl scale --replicas=<number> deployment/<deployment-name>
* To get info of deployment           -----> kubectl get deploy <deployment> -o [ wide | yaml ]

---------------------------------------------- to rollout to different version -----------------------------------------------------------

* To change the version of image      -----> kubectl set image deployment/<deployment-name> <container-name>=<new-image-name>  --record=true
                                               # to get the conainer name try " kubectl get deploy <deployment> -o yaml" and look for spec.containers.name
* To edit deployment through edit 
   deployment file                    -----> kubectl edit deployment/<deployment-name> --record=true
* To get previous version changes made ----> kubectl rollout history deployment/<deployment-name>  # gives u the history of previous roll outs

--------------------------------------------------- to rollback to previous version of deployment ------------------------------------------------

* To check the roll out history         -----> kubectl rollout history deployment/<deployment-name>
* To verify the changes made            -----> kubectl rollout history deployment/<deployment-name> --revision=<number> 
* To rollback to previous version       -----> kubectl rollout undo deployment/<deployment-name>
* To rollback to specific version       -----> kubectl rollout undo deployment/<deployment-name> --to-revision=<number>

---------------------------------------------------------- pause and resume deployments  ------------------------------------------------------

* To pause the deployment              -----> kubectl rollout pause deployment/<deployment-name>
* To make any updates or set resources -----> kubectl set resource deployment/<deployment-name> -c=<container0-name> --limits=cpu=20m,memory=30Mi
* To resume the deployment             -----> kubectl rollout resume deployment/<deployment-name>

----------------------------------------------------- services ----------------------------------------------------------------------

# Available services                        -----> ClusterIP, NodePort, LoadBalancer, Ingress, externalName
# clusterip                                 -----> used for communcation b/w app inside cluster (front and backend)
# NodePort                                  -----> used to access app outside the k8s cluster 
# LoadBalancer                              -----> Primarily to cloud providers for integrate with load balancer
# Ingress                                   -----> An advanced load balancer provide content path based routing (ssl,ssl redirect--etc)
# externalName                              -----> To access external hosted apps in k8s cluster (eg; access aws RDS DATABASE endpoint by an app in k8s cluster)

------------------------------------------------------------------  yaml basics -------------------------------------------------------------------------------

name: sachin                     -------->  {"name":"sachin"}
person:
    name: sachin                 
    age: 24                      --------> {"person":{"name":"sachin", "age":24}}
    hobies:
      - cycling
      - coocking                 --------> {"person":{"name":"sachin", "age":24, "hobies":["cycling", "coocking"]}}
    friends:
      - name: souvik
        age: 24
      - name: ankit              --------> {"person":{"name":"sachin", "age":24, "hobies":["cycling", "coocking"], "friends":[{"name":"souvik", "age":24}, {"name":"ankit", "age": 24}]}}
        age: 24

---  ##  yaml seperator

# second object 
name: ankit                     -------->  {"name":"ankit"}
person:
    name: ankit                
    age: 24                      --------> {"person":{"name":"ankit", "age":24}}
    hobies:
      - cycling
      - coocking                 --------> {"person":{"name":"ankit", "age":24, "hobies":["cycling", "coocking"]}}
    friends:
      - name: souvik
        age: 24
      - name: ankit              --------> {"person":{"name":"ankit", "age":24, "hobies":["cycling", "coocking"], "friends":[{"name":"souvik", "age":24}, {"name":"ankit", "age": 24}]}}
        age: 24


---------------------------------------------------------------------------  pyspark  -------------------------------------------------------------------------

# installing/downloading the required packages
 
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

# creating a pyspark object or a spark session (spark context)

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate() 

# master -->  If you are running it on the cluster you need to use your master name as an argument to master().
# Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0, x value should be the number of CPU cores you have.
# appName() – Used to set your application name.
#getOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist.

#------------------------------------------------------------------- creating an rdd ------------------------------------------------------------------------#

# PySpark RDD Operations
  # RDD transformations – Transformations are lazy operations, instead of updating an RDD, these operations return another RDD.
  # RDD actions – operations that trigger computation and return RDD values.

# Create RDD using sparkContext.parallelize()
  
  #  This function loads the existing collection from your driver program into parallelizing RDD. This is a basic method to create RDD and is used when you 
  #  already have data in memory that is either loaded from a file or from a database. and it required all data to be present on the driver program prior to 
  #  creating RDD.
  
data = [1,2,3,4,5,6,7,8,9,10,11,12]
rdd=spark.sparkContext.parallelize(data)


#Create RDD from external Data source (using sparkContext.textFile())
rdd2 = spark.sparkContext.textFile("/path/textFile.txt")


# Create RDD using sparkContext.wholeTextFiles()
  # wholeTextFiles() function returns a PairRDD with the key being the file path and value being file content.
rdd3 = spark.sparkContext.wholeTextFiles("/path/textFile.txt")


# Creates empty RDD with no partition    
rdd = spark.sparkContext.emptyRDD 
# rddString = spark.sparkContext.emptyRDD[String]


#Create empty RDD with partition
rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions

################### shuffel  in rdd ########################

# getNumPartitions() – This a RDD function which returns a number of partitions our dataset split into.
   # eg --> print("initial partition count:"+str(rdd.getNumPartitions()))
   
# repartition() method which shuffles data from all nodes also called full shuffle 
   # eg ---> reparRdd = rdd.repartition(4)

# coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.

# 
################################## Rdd transformations #########################

# For practice download/copy test.txt file from a git repo --> https://github.com/spark-examples/spark-scala-examples/blob/master/src/main/resources/test.txt

#1 create a rdd
rdd = spark.sparkContext.textFile("path/to/test.txt") 
# This will conver the entire file into a list of strings, where each string is a line in a file 

#2 To read the contents of rdd
print(rdd.collect())

#2 flatMap() -->  It splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record.
rdd2 = rdd.flatMap(lambda x: x.split(" "))
# This will loop over every string in rdd and outputs a list of words in a file

#3 map() transformation is used the apply any complex operations like adding a column, updating a column e.t.c. the output of map transformations would always 
   # have the same number of records as input.
rdd3 = rdd2.map(lambda x: (x,1))

#4 reduceByKey – reduceByKey() merges the values for each key with the function specified. In our example, it reduces the word string by applying the sum 
   # function on value. The result of our RDD contains unique words and their count. 
rdd4 = rdd3.reduceByKey(lambda a,b: a+b)
# It gives the list of tuples which consist of word and its count

#5 sortByKey – sortByKey() transformation #$is used to sort RDD elements on key.
rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()
  # first, we convert RDD[(String,Int]) to RDD[(Int, String]) using map transformation (map(lambda x: (x[1],x[0])))
  # and apply sortByKey which ideally does sort on an integer value
  
#6 filter() transformation is used to filter the records in an RDD.In our example we are filtering all words starts with “a”.
rddf = rdd5.filter(lambda x : 'a' in x[1][0])

#################################### rdd actions ################################

#1 count() – Returns the number of records in an RDD
print("Count : "+str(rdd5.count()))

#2 first() – Returns the first record.
print("first : "+str(rdd5.first()))

#3 max() – Returns max record.
print("max : "+str(rdd5.max()))

#4 reduce() – Reduces the records to single, we can use this to count or sum.

 
#5 take() – Returns the record specified as an argument.
data3 = rdd5.take(3)	# returns A LIST of first 3 records/values

#6 collect() – Returns all data from RDD as an array. 
data=rdd5.collect()

#7 saveAsTextFile() – Using saveAsTestFile action, we can write the RDD to a text file.
rdd6.saveAsTextFile('any') 
# saves the content of rdd6 in a file 'any'

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
---------------------------------------------------------------------------  PRODUCT  -----------------------------------------------------------------------
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

---------------------------------------------------------------------------- MongoDB  -----------------------------------------------------------------------
---------  Things/Terms to know --------

# field     (Coloum)  ---> In MongoDB, a field (also known as a key or attribute) is a fundamental unit of data storage within a document. 

# document (row)     ---> In MongoDB, a document is a basic unit of data storage and the core data representation format. It is equivalent to a record or a row in
                           a traditional relational database management system (RDBMS).

# Collection (TABLE)  ---> In MongoDB, a collection is a grouping of individual documents. It is equivalent to a table in RDBMS

# database            ---> In MongoDB, a database is a container for collections, and it is a fundamental unit for organizing and storing data. 

----------------- installation & basic commands -----------------

# link to install community edition                         ----------> https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-ubuntu/
# link to install mongodb compass; GUI got mangodb          ----------> https://www.mongodb.com/try/download/compass

# To use the mongodb shellfrom ur terminal to interact      ----------> mongosh
# To get info of current db we are working on               ----------> db
# To list all the available databases                       ----------> show dbs
# To list all the available collections                     ----------> show collections
# To use/work on a specified database                       ----------> use <database-name>
# To clear the screne                                       ----------> cls


----------- create operation -----------
      
  !!! NOTE : COMMANDS ARE WRITTEN IN MONGODB SHELL !!! 
                                                        |---> db.<collection-name>.insertOne({<keyname1>:<value1>,<keyname2>:<value2>})
# To create a collecton in database use the syntax   ---|                                      
                                                        |---> db.<collection-name>.insertMany([{<keyname1>:<value1>},{<keyname2>:<value2>},{<keyname3>:<value3>,{<keyname4>:<value4>}])

     * insertOne() is used when we want to insert a single document to a collecton

     * insertMany() is is used when we want to insert a multiple documents to a collecton

-------------- Read/Find the data and project only required -----------

# To read all the document inside collection     ---- > db.<collection-name>.find() (or) db.<collection-name>.find({})
# To read a perticular document from collection  -----> db.<collection-name>.find({<Key-Name>:<ValueName>})
# TO find a single document from collection      -----> db.<collection-name>.findOne({<Key-Name>:<ValueName>})
# To project only required data                  -----> db.orders.aggregate([ {$group: {_id:"$product", total_qty: {$sum: "$quantity"}, {$project: {_id: 0, product: "$_id",total_qty:1}} ])
                                                                                                                                        ---------------------------|-------------------------
-------------- Update ---------------------                                                                       projrcting req data : 0 means hide, 1 for show <-|                                                    

# To modify the single document in a collection  -----> db.<collection-name>.updateOne( {<feildName>: <Value>},{ $set : {<field-name>: <new-value>},$currentDate : {lastmodified: true}})
                                                                                         ----------|-----------        --------------|------------- ---------------------|----------------
                                                                                                   |                                 |                                   |-> Optional
                                                                                (To identify the DOCUMENT that need to change)     (To select and update the new-value)   
                                                                                                    |                                |                           
                                                                                        ------------|---------          -------------|--------------
# To modify the single document in a collection  -----> db.<collection-name>.updateMany( {<feildName>: <Value>},{ $set : {<field-name>: <new-value>},$currentDate : {lastmodified: true}})
   
      * Uses the  $set operator to update the value of the plot field for the movie Twilight.
      * Uses the $currentDate voperator to update the value of the lastUpdated field to the current date. If lastUpdated field does not exist, it will create.

--------------- delete ---------------------

* To delete a single collection                 ------> db.<collection-name>.deleteOne( {<field-name> : <value>} ) 
                                                                                        -------------|------------
                                                                                                     |
                                                                                      (Identifier to delete a selected value)
                                                                                                     |
                                                                                          -----------|-------------           
* To delete multiple collection                 ------> db.<collection-name>.deleteMany( {<field-name> : <value>} )

* To dlete all tehe values                      ------> db.<collection-name>.deleteMany( {} )


----------------- operators ----------------

!!!!!! NOTE : FOR BELOW OPERATIONS CONSIDER THE EXAMPLE  !!!!!!!!!

db.users.insertMany(
[
  {
    "_id": ObjectId("61368a82b77b650fde1ca27f"),
    "name": "John Doe",
    "age": 30,
    "email": "john.doe@example.com",
    "address": {
      "city": "New York",
      "zip": "10001"
    },
    "hobbies": ["reading", "cooking", "gaming"]
  },
  {
    "_id": ObjectId("62348b87c82a240cdf0ca58d"),
    "name": "Jane Smith",
    "age": 25,
    "email": "jane.smith@example.com",
    "address": {
      "city": "Los Angeles",
      "zip": "90001"
    },
    "hobbies": ["painting", "gaming", "hiking"]
  },
 ]
)

* Comparison operators:
   
     * Equal to                 ------> db.users.find({ age: { $eq: 30 } })
     * Notequal to              ------> db.users.find({ age: { $ne: 30 } })
     * Greaer than              ------> db.users.find({ age: { $gt: 30 } })
     * Greaer than equal to     ------> db.users.find({ age: { $gte: 30 } })
     * Lesser than              ------> db.users.find({ age: { $lt: 30 } })
     * Lesser than equal to     ------> db.users.find({ age: { $lte: 30 } })

* Logical Oprators
    
     * $and: Joins query clauses with a logical AND. ----> db.users.find({ $and: [{ age: { $gte: 25 } }, { age: { $lte: 35 } }] })
     * $or: Joins query clauses with a logical OR.   ----> db.users.find({ $or: [{ age: { $lt: 25 } }, { age: { $gt: 35 } }] })
     * $not: Inverts the result of a query.          ----> db.users.find({ age: { $not: { $eq: 30 } } })

* Array Operators:

     * $in : Matches any of the values specified in an array.        ----> db.users.find({ hobbies: { $in: ["reading", "cooking"] } })
     * $nin: Matches none of the values specified in an array.       ----> db.users.find({ hobbies: { $nin: ["gaming", "swimming"] } })
     * $all: Matches arrays that contain all the specified elements. ----> db.users.find({ hobbies: { $all: ["reading", "cooking"] } })
     * $elemMatch: Matches documents that contain an array element   ----> db.users.find({ hobbies: { $elemMatch: { $eq: "reading", $ne: "cooking" } } }) 
       that matches all the specified criteria.
    
* Update Operators:
  
     * $set: Sets the value of a field in an update operation.     ----> db.users.updateOne({ _id: ObjectId(<obj-id>) }, { $set: { age: 31 } })
     * $unset: Removes a field from a document.                    ----> db.users.updateOne({ _id: ObjectId(<obj-id>) }, { $unset: { address: "" } })
     * $inc: Increments the value of a field by a specified amount.----> db.users.updateOne({ _id: ObjectId(<obj-id>  ) }, { $inc: { age: 1 } })
     * $push: Adds an element to an array.                         ----> db.users.updateOne({ _id: ObjectId(<obj-id>) }, { $push: { hobbies: "painting" } })

* Aggregation Operators: # still have to work on it
* Projection Operators:  # still have to work on it

--------------------- Clauses in mongodb -----------------------

In mongoDB, there are no special cases for where, groupby, having, orderby clauses as in traditional RDBMS
Here we use aggrigate function and we pass combination of functions to perform a task of where, groupby, having, orderby in mongoDB

!!!! CONSIDER THE EXAMPLE TO WORK WITH !!!!!!!

db.orders.insertMany([
  {
  "_id": 1,
  "customer": "John",
  "product": "Widget",
  "quantity": 3,
  "total_price": 75
},
{
  "_id": 2,
  "customer": "Alice",
  "product": "Gadget",
  "quantity": 2,
  "total_price": 100
},
{
  "_id": 3,
  "customer": "John",
  "product": "Gadget",
  "quantity": 1,
  "total_price": 50
},
{
  "_id": 4,
  "customer": "Bob",
  "product": "Widget",
  "quantity": 5,
  "total_price": 125
}
])

## where clause 
  ! SQL     ---> SQL: SELECT * FROM orders WHERE total_price > 60;
  ! Mongodb ---> db.orders.aggregate([ {$match: {total_price : {$gt : 60}}}])

## Groupby Clause
  ! SQL     ---> SQL: SELECT product, SUM(quantity) AS total_quantity, SUM(total_price) AS total_price FROM orders GROUP BY product;
  ! MongoDB ---> db.orders.aggregate([ { $group: { _id: "$product",total_quantity: {$sum: "$quantity"}, total_price: { $sum: "$total_price"}}} ])

##  Having Clause
  ! SQL     ---> SELECT product, SUM(quantity) AS total_quantity FROM orders GROUP BY product  HAVING SUM(quantity) > 5;
  ! MongoDB ---> db.orders.aggregate([ { $group: { _id: "$product", total_quantity: { $sum: "$quantity"} }}, { $match: { total_quantity: { $gt: 5}}} ])


## Orderby Cluse
  ! SQL    ---> SQL: SELECT * FROM orders ORDER BY total_price DESC;
  ! MongoDB --> db.orders.aggregate([{ $sort: { total_price: -1 }}])

## Distinct
  ! To find list of distict values in collection  ----> db.<collection>.distinct({<field-name> : <value>})
  ! To find combination of distint values         ----> db.<collection>.distinct({<field-name> : <value>}, {<field-name1> : <value1>})

## To get only req o/p in aggregation pipeline
  ! db.orders.aggregate([ { $group: { _id: "$customer" } }, { $project: { _id: 0,  customer: "$_id"} }] )
                                                             ## _id: 0             ----> Exclude the _id field*/
                                                             ## customer: "$_id"   ----> Rename the _id field to customer

------------------------------- mongodump and mongorestore ------------------------

!!!!! NOTE : TO RESTORE, THE SOURCE HAS TO BE DELETED IN MONGODB, IF SOURCE IS PRESENT, THE RESTORATION FAILS   !!!!!

# To dump all databases                 ----> mongodump
# To restore all databases              ----> mongorestore
# To dump a single database             ----> mongodump  -d <dbname> --out /path/to/folder
# To restore a single database          ----> mongorestore path/to/source/folder
# To dump single collection             ----> mongodump -d <db-name> -c <Collection-name> --out /path/to/folder
# To restore a single collection        ----> mongorestore path/to/source/folder
# To restore a specific data            ----> mongodump -d <db-name> -c <Collection-name> --query '{<yourquery>}' --out /path/to/folder
                                              Eg: mongodump -d Employees -c orders --query '{"total_price": {"$gt": 60}}' --out Devops
# To restore that specific data         ----> mongorestore path/to/source/folder


-------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                           MYSQL
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Install mysql on ubuntu       --------> https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-22-04
# Install mysql-workbench-comm  --------> sudo snap install mysql-workbench-community
# Login to ur server            --------> mysql -u sac -p
# To check status of mysql-server ------> systemctl is-active mysql
# To check the version          --------> sudo mysqladmin -p -u sac version
# TO clear screen               --------> ctrl+l
# Datatypes in mysql            --------> CHAR(size)        => max size 2000char, fixed allocation of memory
                                          VARCHAR(size)     => max size 4000char, variable allocation of memory
                                          NUMBER(total_digits,decimal_places) => eg; Number(4,2) ranges (00.00 - 99.99), means out of 4 places 2 are decimals
                                          DATE              => used to store date values, default format is 'YYYY-MM-DD'.
                                          # There are many more, the above are the important
                                          
# Constraints in mysql          --------> UNIQUE       => It will not allow to add  duplicate/repeated values in a coloum
                                          NOT NULL     => Does not allow to leave/fill the cell with null value 
                                          CHECK        => Accepts if given condition satisfies else not eg; CHECK (product_price > 0)
                                          PRIMARY KEY  => combo of not null and check
                                          FOREIGN KEY  => Used to establish connection between tables
                                          DEFAULT      => If user not enters data, used to set the default data eg;  hire_date DATE DEFAULT CURRENT_DATE,
----------------( bAsic OperaTionS )----------------------

# To list all the databases                      ----> SHOW DATABASES;
# To list all tabels in the databases            ----> SHOW TABLES;
# To shows information on all columns of a table ----> DESCRIBE <Table-Name>;
# To  provides even more details on the table    ----> SHOW CREATE TABLE <tab-name>\G
# To creata a mew database                       ----> CREATE DATABASE <db-Name>;
# To select a database to work on                ----> USE <db-name>;

---------------- SQL STATEMENTS AND COMMANDS -------------
     STATEMANT                  ----> COMMANDS
# Data Query Lang (DQL)         ----> select
# Data Defination Lang (DDL)    ----> create, rename, truncate, alter, drop
# Data Manipulation Lang (DML)  ----> Insert, update, delete
# Transition Control Lang (TCL) ----> rollback, commit, savepoint
# Data Control Lang (DCL)       ----> Grant, revoke

----------------------------------------------------------------------- CRUD OPERATIONS -------------------------------------------------------------

# To create a table inside database              ----> CREATE TABLE <TAB-NAME> ( C1 D1 CO1, C2 D2 CO2 ---- CN DN CON, PRIMARY KEY  (PK-COL-NAME) );
                                                                      # C1,C2,---CN : Coloumn Names
                                                                      # D1,D2,---DN : Data Types for respective coloumn
                                                                      # CO1,CO2--CON: Constrains for  respective coloumn
                                                                      
# To add records/values to table                 ----> INSERT INTO <TAB-NAME> ( C1, C2,-- CN) VALUES ( A1, A2, -- A3 ), (B1, B2, -- BN), -- (N1, N2, -- NN);
                                                                      # C1,C2,---CN : Coloumn Names
                                                                      # A1,A2,---AN : vALUES OF OBJECT A
                                                                      # B1,B2,--BN  : VALUES OF OBJECT B  
                                                                      
# Basic syntax of select statement               ----> SELECT * | [DISTINCT] <COL-NAME> | <EXPRESSION> [AS 'ALIAS'] FROM <DB-NAME>.<TAB-NAME>;
                                                                      # |       -> Or (Which means either any one of the following)
                                                                      # []      -> Content is not necessary, can be excluded
                                                                      # 'ALIAS' -> ('') If it does not contain any special charecter
                                                                      # "ALI@S" -> ("") If it contains special charecters
                                                                      # DISTINCT-> If we want to exclude repeted values 
                                                                                   eg: SELECT DISTINCT sal FROM FROM <DB-NAME>.<TAB-NAME>;
                                                                      
# To select all values from table                ----> SELECT * FROM <DB-NAME>.<TAB-NAME>;
# To select distinct value from table            ----> SELECT DISTINCT <COLUMN-NAME> FROM <DB-NAME>.<TAB-NAME>;
# To give alias name to a coloum                 ----> SELECT <COLOUMN-NAME> AS '<ALIAS-NAME>' FROM <DB-NAME>.<TAB-NAME>;
# To use expression in an select statemant       ----> SELECT <EXPRESSION> FROM <DB-NAME>.<TAB-NAME>;
                                                                      # for eg; query to get annual salary of all the emplyees with an alias name anual_sal
                                                                        SELECT *, sal*12 AS "anual_sal" FROM <DB-NAME>.<TAB-NAME>;

------------------------------------------------------------------------    MYSQL BASICS  -----------------------------------------------------------

Consider the below table as an example for practice

    -------------------------------------------------------------------------------
    |    ename      |   job     |MgrID|  Hirdate   |   sal   | commission| deptno   |   
    --------------------------------------------------------------------------------
    |'John Smith'   | 'manager' | NULL| '2023-01-15'| 6000.00| NULL      |   10     |
    |'Jane Doe'     | 'manager' | NULL| '2023-02-10'| 6200.00| NULL      |   20     |
    |'Alice Johnson'| 'clerk'   | 1   | '2023-03-05'| 3000.00| NULL      |   20     |
    |'Bob Brown'    | 'clerk'   | 1   | '2023-04-20'| 3200.00| NULL      |   20     |
    |'Charlie Wison'| 'analyst' | 2   | '2023-05-15'| 7000.00| NULL      |   30     |
    |'David Lee'    | 'salesman'| 3   | '2023-06-10'| 5000.00| 200.00    |   30     |
    |'Eva Martinez' | 'salesman'| 3   | '2023-07-25'| 5200.00| 250.00    |   30     |
    |'Frank Harris' | 'manager' | NULL| '2023-08-20'| 6400.00| NULL      |   10     |
    |'Grace Turner' | 'clerk'   | 8   | '2023-09-05'| 3100.00| NULL      |   10     |
    |'Helen Davis'  | 'analyst' | 2   | '2023-10-10'| 7200.00| NULL      |   30     |
    |'Ivy White'    | 'salesman'| 7   | '2023-11-15'| 5100.00| 220.00    |   10     |
    |'Jack Wilson'  | 'clerk'   | 1   | '2023-12-20'| 3300.00| NULL      |   20     |
    |'Karen Brown'  | 'manager' | NULL| '2024-01-15'| 6300.00| NULL      |   30     |
    |'Larry Green'  | 'clerk'   | 1   | '2024-02-10'| 3100.00| NULL      |   10     |
    |'Mia Miller'   | 'analyst' | 11  | '2024-03-05'| 7100.00| NULL      |   20     |
    |'Nora Johnson' | 'salesman'| 3   | '2024-04-20'| 5300.00| 240.00    |   10     |
    |'Oliver Clark' | 'manager' | NULL| '2024-05-15'| 6500.00| NULL      |   20     |
    |'Paula Andrson'| 'analyst' | 15  | '2024-06-10'| 7300.00| NULL      |   30     |
    |'Quincy Turner'| 'salesman'| 13  | '2024-07-25'| 5400.00| 260.00    |   30     |
    |'Rachel Harris'| 'clerk'   | 8   | '2024-08-20'| 3200.00| NULL      |   20     |
    ---------------------------------------------------------------------------------



------------------------------------------------------------------------  DUMP AND RESTORE -----------------------------------------------------------

# TO Dump an entier database                     ----> mysqldump -u <username> -p <dbname> > /op/file.sql
# To dump a table from database                  ----> mysqldump -u <username> -p <dbname> <tablename> > /op/file.sql
# To dump a selected data from database          ----> mysqldump -u <username> -p <dbname> <tablename> -w "<condition>" > op/file.sql
# To restore the database                        ----> mysql -u root -p <dbname> < source/file.sql
# To restore table in database                   ----> mysql -u root -p <dbname> < source/file.sql
# To restore selected data from database         ----> mysql -u root -p <dbname> < source/file.sql


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
                                                                          HADOOP
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# Installation link                              ----> https://tecadmin.net/how-to-install-apache-hadoop-on-ubuntu-22-04/
# To start the Hadoop cluster                    ----> start-all.sh
# To stop the clster                             ----> stop-all.sh
# To list all commands                           ----> h	dfs
# Hadoop, Application page                       ----> http://localhost:9870, http://localhost:8088

------------------------------------------------------------------------------------------------------------------------------------------------------------

# To copy file from local to hdfs                     ----> hdfs dfs -copyFromLocal <source> <destination> (or) hdfs dfs -put <source> <destination>
# To move file from local to hdfs                     ----> hdfs dfs -moveFromLocal <source> <destination
# To copy from hdfs to local                          ----> hdfs dfs -copyToLocal <source> <destination>
# To check disk usage of each file in directory       ----> hdfs dfs -du <folder>
# To check disk usage of entier directory             ----> hdfs dfs -dus <folder>
# To merge content of multiple files to singl file    ----> hdfs dfs -getmerge <source1> <source2> <destination>
# To create a parent directory that doesnt exist      ----> hdfs dfs -mkdir -p /user/<UserName>/<DirName>  # -p indictes to create any parent directories that don't exist
# To check weather it is a file or not                ----> hdfs dfs -test -f  /user/<Username>/path/to/file && echo $? -----|
# To check weather it is a directory or not           ----> hdfs dfs -test -d  /user/<Username>/path/to/file && echo $? -----|--| 0 is true
# To check weather file/directory it exist or not     ----> hdfs dfs -test -e  /user/<Username>/path/to/file && echo $? -----|--| 1 is false
# To check weather it has content or not              ----> hdfs dfs -test -z  /user/<Username>/path/to/file && echo $? -----|
# command 2 report on missing, under-replicated, and  
  corrupted blocks.                                   ----> hdfs fsck / -files -blocks -locations
# To get first 1kb content of file                    ----> hdfs dfs -head /user/<Username>/path/to/file
# To get last 1kb content of file                     ----> hdfs dfs -tail /user/<Username>/path/to/file
# To delete all the file in trash                     ----> hdfs dfs -expunge
# To balance data equally in all nodes of cluster     ----> hdfs balance




